## 저수준 API

대부분의 상황에서는 구조적 API를 사용하는 것이 좋다. 하지만 비즈니스나 기술적 문제를 구수준 API를 삳용해 모두 처리할 수 있는 것은 아니므로, 이런 상황에서 스파크의 저수준 API를 사용해야 한다.

스파크의 저수준 API는 RDD, SparkContext, Accumulator, Broadcast Variable 같은 분산형 공유 변수 distributed shared variables 등을 의미한다.



## 12.1 저수준 API란

스파크에는 두 종류의 저수준 API가 있다.

- 분산 데이터 처리를 위한 RDD
- 분산형 공유 변수를 배포하고 다루기 위한 브로드캐스트 변수와 어큐뮬레이터



## 12.1.1 저수준 API는 언제 사용할까

다음과 같은 상황에서 저수준 API를 사용한다.

- 고수준 API에서 제공하지 않는 기능이 필요한 경우. 예를 들어 클러스터의 물리적 데이터의 배치를 아주 세밀하게 제어해야 하는 상황
- RDD를 사용해 개발된 기존 코드를 유지해야 하는 경우
- 사용자가 정의한 공유 변수를 다뤄야 하는 경우

위와 같은 상황에서만 저수준 API 기능을 사용해야 한다. 그러나 스파크의 모든 워크로드는 저수준 기능을 사용하는 기초적인 형태로 컴파일되므로 이를 이해하는 것은 많은 도움이 된다.

DataFrame 트랜스포메이션을 호출하면 실제로 다수의 RDD 트랜스포메이션으로 변환된다. 이러한 관계를 이해하면 점점 더 복잡해지는 워크로드를 디버깅하는 작업이 쉬워질 것이다.

스파크를 잘 알고 있는 숙련된 개발자라 하더라도 구조적 API 위주로 사용하는 것이 좋다. 그러나 필요한 요건을 맞추기 위해 저수준 API를 사용해야 하는 경우가 발생할 수 있다.

이전 버전의 스파크에서 자체 구현한 파티셔너를 사용하거나 데이터 파이프라인이 실행되는 동안 변수의 값을 갱신하고 추적하는 데 저수준 API가 필요할 수 있다.

저수준 API는 세밀한 제어 방법을 제공하여 개발자가 치명적인 실수를 하지 않도록 도와주기도 한다.



## 12.1.2 저수준 API는 어떻게 사용할까

`SparkContext`는 저수준 API 기능을 사용하기 위한 진입 지점이다. 스파크 클러스터에서 연산을 수행하는 데 필요한 도구인 `SparkSession`을 이용해 `SparkContext`에 접근할 수 있다.

- 다음 명령을 사용해 `SparkContext`에 접근이 가능하다.

```python
spark.sparkContext
```



---

## 12.2 RDD 개요

RDD는 스파크 1.x 버전의 핵심 API이다. 하지만 이후 버전인 스파크 2.x 버전에서는 잘 사용하지 않는다. DataFrame, Dataset는 사용자가 실행 시 RDD로 컴파일된다. 또한 스파크 UI에서 RDD 단위로 잡이 수행됨을 알 수 있다. 그러므로 RDD가 무엇인지, 어떻게 사용하는지 기본적으로 이해하고 있어야 한다.

RDD는 간단히 말해 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음이다. DataFrame의 각 레코드는 스키마를 알고 있는 필드로 구성된 구조화된 로우인 반면, RDD의 레코드는 그저 프로그래머가 선택하는 자바, 스칼라, 파이썬의 객체일 뿐이다.



















