{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pyspark\n",
    "import time\n",
    "import datetime\n",
    "import configparser\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, TimestampType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.context import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/19 21:37:45 WARN Utils: Your hostname, yangws.local resolves to a loopback address: 127.0.0.1; using 192.168.0.8 instead (on interface en9)\n",
      "22/04/19 21:37:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/04/19 21:37:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--num-executors 96 --total-executor-cores 96 --driver-memory 128g --executor-memory 4g pyspark-shell\"\n",
    "\n",
    "driver_memory = '2g'\n",
    "executor_memory = '1g'\n",
    "pyspark_submit_args = ' --driver-memory=' + driver_memory + ' --executor-memory=' + executor_memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "spark = SparkSession(sc).builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .appName(\"attrubution semi\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = spark.read.option(\"multiline\", \"true\").json(\"./test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_schema = StructType([\n",
    "    StructField('action type', StringType(), True),\n",
    "    StructField('engine', StringType(), True)\n",
    "])\n",
    "\n",
    "# OR\n",
    "\n",
    "tmp_schema = (StructType()\n",
    "              .add('action type', StringType(), True)\n",
    "              .add('engine', StringType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_df = spark.read.schema(tmp_schema).option('multiline', 'true').json('./test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|action type|engine|\n",
      "+-----------+------+\n",
      "|       노출|moloco|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = schema_df.select(col('action type').alias('action_type'), col('engine')).alias('copy_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|action_type|engine|\n",
      "+-----------+------+\n",
      "|       노출|moloco|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_df.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "709125bc3324568166aaae73d1ec683bc8fb754a7ca0393859952f5a67b588b5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
