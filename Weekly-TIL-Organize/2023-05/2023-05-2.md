<aside> 💡 업무 정리에 대해 더 생각하게 만든 한 주였다. 또한 ChatGPT를 사용하여 질문하고 답변에 대해 공부하는 것에 매우 즐거움을 느낀 한 주이기도 했다.

</aside>

- [ ]  AWS Glue와 비슷한 서비스들을 살펴보기(e.g. AWS Batch)
    - 면접 스크립트를 생성하기 위한 작업
- [ ]  Kafka의 Consumer 종류들에 대한 간략한 앎이 필요
    - Connector, Streaming Consumer 등
- [ ]  [LeetCode(Coding Test Site)](https://leetcode.com/problemset/database/?difficulty=MEDIUM&page=1)
- [ ]  새로운 Workflow Tool 알아보기(Prefect, [mage.ai](http://mage.ai))
- [ ]  [Spark에서 read할 때 쓰이는 Glob Pattern에 대해 자세히 알아보기](https://kb.databricks.com/en_US/scala/pattern-match-files-in-path)
- [ ]  Python GIL에 대한 보다 깊은 이해
- [ ]  스터디 참가([Pseudo Lab](https://www.notion.so/May-3e93a38c31ac4a478cdfd45c7504a46d?pvs=21))
- [ ]  Starcoder 알아보기

### 글쓰기

- AwsGlueJobOperator의 Task.output으로 어떻게 Task의 반환값을 가져올 수 있는가

- Spark, Airflow와 같은 곳에서 자주 쓰이는 메서드들이 실제로 어떻게 작동하는지 내부 로직들을 분석하여 정기적으로 발행해보는 것이 재미있을 것 같다.

- Python code to Byte code

    아래 예시는 `print("Hello, World")`라는 간단한 Python 코드를 바이트코드로 변환하는 과정을 보여줍니다.

    1. 먼저 Python 소스 코드를 작성하고 파일에 저장합니다. 여기에서는 `hello.py`라는 파일에 코드를 작성하겠습니다.

    ```
    # hello.py
    print("Hello, World")
    ```

    1. Python 코드를 바이트코드로 컴파일하려면, 터미널에서 `compileall` 모듈을 사용하여 `.pyc` 파일을 생성할 수 있습니다. 다음 명령을 사용합니다.

    ```
    python -m compileall hello.py
    ```

    이 명령을 실행하면, `__pycache__`라는 폴더가 생성되고 그 안에 `hello.cpython-XX.pyc`라는 파일이 생성됩니다 (XX는 파이썬 버전을 나타냅니다).

    1. 이제 생성된 바이트코드 파일에서 바이트코드를 확인해보겠습니다. Python 인터프리터에서 `dis` 모듈을 사용하여 바이트코드를 분석할 수 있습니다.

    ```
    import dis
    import hello
    
    dis.dis(hello)
    ```

    위 코드를 실행하면, 다음과 같은 바이트코드가 출력됩니다.

    ```
      2           0 LOAD_NAME                0 (print)
                  2 LOAD_CONST               0 ('Hello, World')
                  4 CALL_FUNCTION            1
                  6 POP_TOP
                  8 LOAD_CONST               1 (None)
                 10 RETURN_VALUE
    ```

    이렇게 Python 코드가 바이트코드로 변환되고, Python 가상 머신에서 실행되어 결과가 출력됩니다. 이 과정에서 Python 인터프리터 (여기서는 CPython)가 소스 코드를 읽고, 분석하고, 바이트코드로 변환한 다음 실행합니다.

### Information

- Spark에서 NaN과 Null은 다른 값이다.

    - NaN: 수치형 타입에서 쓰이는 빈 값
    - Null: 일반적으로 모든 타입에서 쓰이는 빈 값

- ANTLR Parser

    - 언어 인식을 위한 파서 생성 도구로, 언어 구문 분석 및 처리를 위한 퍼서를 생성하는데 사용한다.
    - ANTLR은 특히 프로그래멍 인어, 데이터 직렬화 형식, 쿼리 언어 등 다양한 도메인에서 사용된다.
    - 주요 기능 및 특징
        1. 언어 독립성
        2. LL(*) 파싱 알고리즘
        3. 쉬운 문법 작성
        4. 오류 처리 및 복구
        5. 추상 구문 트리 생성
    - PySpark에서 ANTLR의 역할
        - SQL 쿼리와 표현식을 구문 분석하는데 사용한다.

- CPython

    - 주요 기능
        1. 소스 코드 분석: Python code를 읽고, 토큰화하고, 추상 구문 트리(AST)를 생성
        2. Byte code 컴파일: 생성된 AST를 바이트코드로 컴파일. 바이트코드는 PVM이 이해할 수 있는 저수준 명령어 집합이다.
        3. Byte code 실행: 컴파일된 바이트코드는 CPython 인터프리터 내의 PVM에서 실행된다. 이 프로세스에서 Python 코드는 실제로 실행되어 결과를 생성한다.

    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/88b6fab5-aa6a-4795-b3e2-f2f8fae12151/Untitled.png)

- Cython

    - C와 유사한 문법으로 함수를 작성하고, 이를 CPython 패키지 형태로 만들어준다.

- Jython

    - JVM을 활용하는 Python 구현체 중 하나로, Python Syntax로 작성된 코드를 JVM이 이해할 수 있는 바이트코드로 만들고, 이 바이트코드를 JVM이 실행하게 된다.
    - Jython의 특징으로 Variable Couting을 사용하는 것이 아니라 JVM에서 사용되는 GC로 구현되어 있기 떄문에 GIL을 사용하지 않는다는 장점이 있다.

- 컴파일러 구현에 따른 프로그래밍 언어의 구분

    1. self-hosted compiler: 자신의 언어로 컴파일러를 작성한 언어(e.g. Go)
    2. source-to-source compiler: 기존에 널리 사용되는 언어를 활용하여 컴파일러를 작성한 언어(e.g. Python)

- PySpark Method

    > `lag(col: ColumnOrName, offset: int = 1, default = Optional[Any] = None) -> col`
    >
    > - Window function: 지정한 col의 offset만큼 전에 해당하는 row의 col을 반환한다. 만약 이전 값이 없다면 null을 반환하며, default를 사용하여 null을 대신하여 특정한 값을 지정할 수 있다.

    > `lead(col: ColumnOrName, offset: int = 1, default: Optional[Any] = None) -> col`
    >
    > - Window function: row 단위로 동작하며, 전달한 window를 기준으로 `offset`만큼 뒤에 해당하는 row의 `col` 값을 반환한다. 만약 값이 없다면 null을 반환하며, `default`를 사용하여 null을 대신해 기본값을 지정할 수 있다.

    > `first(col: ColumnOrName, ignorenulls: bool = False) -> col`
    >
    > - Aggregation function: group에 속한 col의 첫번째 값을 반환

    > `slice(x: ColumnOrName, start: Union[ColumnOrName, int], length: Union[ColumnOrName, int]) -> col`
    >
    > - x는 배열이며, start를 기준 인덱스로 설정하고 lenght까지 x의 요소들을 반환한다.

    > `array_position(col: ColumnOrName, value: Any) -> col`
    >
    > - col은 배열이며, 배열 내의 값들 중 맨 처음 존재하는 value에 해당하는 index를 반환한다. 만약 배열 내에 value가 존재하지 않는다면 0을 반환한다.

    > `exists(col: ColumnOrName, f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> col`
    >
    > - col을 기준으로 f에 따라 true나 false를 반환한다.

    > `broadcast(df: DataFrame) -> DataFrame`
    >
    > - df를 broadcast join으로 사용하도록 명시적으로 선언하는 함수이다. 만약 함수를 사용하지 않을 경우는 spark.sql.autoBroadcastJoinThreshold 설정 값을 기준으로 결정되며, 기본값은 10MB이다. DataFrame의 크기가 이 설정 값보다 작을 경우, Spark는 자동으로 브로드캐스트 조인을 수행한다.

    > `isnan(col: ColumnOrName) -> Column`
    >
    > - col의 값이 NaN일 경우 true, false를 반환한다.
    > - NaN은 Spark의 수치형 타입에서 빈 값을 의미한다.

    - ```
        monotonically_increasing_id() -> Column
        ```

        - DataFrame의 row에 id라는 컬럼명으로 순차적인 수를 넣는다.
        - id컬럼의 값은 고유하다.

    - ```
        nanvl(col1: ColumnOrName, col2: ColumnOrName) -> Column
        ```

        - `col1`의 값이 NaN이면 `col2`를 반환한다.

    - ```
        rand(seed: Optional[int] = None) -> Column
        ```

        - 균일 분포(uniform)를 따르는 랜덤한 수치형 값을 가진 `rand` 컬럼을 반환한다.

    - ```
        randn(seed: Optional[int] = None) -> Column
        ```

        - 표준 정규분포(normal)를 따르는 랜덤한 수치형 값을 가진 `randn` 컬럼을 반환한다.

    - ```
        spark_partition_id() -> Column
        ```

        - 레코드별로 해당하는 partition의 id를 컬럼으로 반환한다.
        - 큰 DataFrame은 각기 row가 여러 partition에 나뉘어 있으므로 해당 row가 속하는 partition의 id를 따로 가지는 것이다.

    - ```
        expr(str: str) -> Column
        ```

        - `str`를 분석하여 표현한다. `str`에 들어오는 값은 SQL 표현식이어야 한다.
        - SQL 표현식은 ANTLR Parser에 의해 분석된다.

    - ```
        greatest(*cols: ColumnOrName) -> Column
        ```

        - `cols`에 전달된 column 중 가장 큰 값을 가진 column을 반환한다.
        - 적어도 2개 이상의 `cols`가 전달되어야 한다.
        - `cols`에 전달된 column들의 값이 모두 null일 경우 null을 반환한다.

    - ```
        least(*cols: ColumnOrName) -> Column
        ```

        - `cols`에 전달된 column 중 가장 작은 값을 가진 column을 반환한다.
        - 적어도 2개 이상의 column이 `cols`에 전달되어야 한다.
        - `cols`에 전달된 column들이 모두 null일 경우 null을 반환한다.

    - ```
        monotonically_increasing_id() -> Column
        ```

        - DataFrame의 row에 id라는 컬럼명으로 순차적인 수를 넣는다.
        - id컬럼의 값은 고유하다.

    - ```
        nanvl(col1: ColumnOrName, col2: ColumnOrName) -> Column
        ```

        - `col1`의 값이 NaN이면 `col2`를 반환한다.

    - ```
        rand(seed: Optional[int] = None) -> Column
        ```

        - 균일 분포(uniform)를 따르는 랜덤한 수치형 값을 가진 `rand` 컬럼을 반환한다.

    - ```
        randn(seed: Optional[int] = None) -> Column
        ```

        - 표준 정규분포(normal)를 따르는 랜덤한 수치형 값을 가진 `randn` 컬럼을 반환한다.

    - ```
        spark_partition_id() -> Column
        ```

        - 레코드별로 해당하는 partition의 id를 컬럼으로 반환한다.
        - 큰 DataFrame은 각기 row가 여러 partition에 나뉘어 있으므로 해당 row가 속하는 partition의 id를 따로 가지는 것이다.

    - ```
        expr(str: str) -> Column
        ```

        - `str`를 분석하여 표현한다. `str`에 들어오는 값은 SQL 표현식이어야 한다.
        - SQL 표현식은 ANTLR Parser에 의해 분석된다.

    - ```
        greatest(*cols: ColumnOrName) -> Column
        ```

        - `cols`에 전달된 column 중 가장 큰 값을 가진 column을 반환한다.
        - 적어도 2개 이상의 `cols`가 전달되어야 한다.
        - `cols`에 전달된 column들의 값이 모두 null일 경우 null을 반환한다.

    - ```
        least(*cols: ColumnOrName) -> Column
        ```

        - `cols`에 전달된 column 중 가장 작은 값을 가진 column을 반환한다.
        - 적어도 2개 이상의 column이 `cols`에 전달되어야 한다.
        - `cols`에 전달된 column들이 모두 null일 경우 null을 반환한다.

- NumPy가 Python보다 빠른 이유

    1. Python의 순서형 데이터 객체들은 컴퓨터 메모리에 연속되지 않은 곳에 저장되는 반면, NumPy의 Array 객체는 메모리에 연속적으로 저장되어 메모리에 접근하는 속도에서 차이가 발생한다.
    2. NumPy는 계산할 때 처리해야 할 계산들을 복수 개의 프로세스로 나눈 후 병렬로 연산한다.
    3. NumPy의 함수는 C/C++로 만들어져 Python 내장함수에 비해 수행하는 명령어들이 적다.

- Airflow Operator

    > 아래의 설정들은 Operator가 작업을 수행하는 방식에 영향을 미친다. 템플릿화 된 필드는 Operator가 실행되기 전 Jinja Template Engine에 의해 처리되므로, 이를 통해 동적으로 필드 값을 설정할 수 있다.

    1. `template_fields`: 이 튜플에 포함된 필드는 템플릿화된다. 이 필드들은 Jinja Template Engine에 의해 파싱되고 처리된다.
    2. `template_ext`: Operator가 처리할 수 있는 템플릿 파일의 확장자를 나타내는 튜필이다. Jinja Template 형식을 사용하여 파일의 경로를 읽을 수 있도록 설정하는 것이다.
    3. `template_fields_renderers`: Dict 타입으로 구성되어 있으며, Airflow의 WebUI에서 어떻게 템플릿 필디를 렌더링할 지 결정한다.

### Read this.

- [29CM ML Pipeline](https://aws.amazon.com/ko/blogs/tech/29cm-sagemaker-mwaa-recsys-mlops-journey/)
- [OpenAi, Sam Artman - 원격 근무는 끝났다.](https://news.hada.io/topic?id=9125&utm_source=slack&utm_medium=bot&utm_campaign=T03G8V62N)
- [Rust를 사랑하는 이유](https://zdnet.co.kr/view/?no=20200122110100)

### 운영업무 중 발생한 이슈

- Airflow에서 발생한 이슈이다. Glue Job은 Succeed하였는데 UI 상의 Task의 값은 Failed로 처리되어 있다. 어떤 상황이기에 발생했던 것일까?

### Thinking

- 업무 정리에 대한 고찰
    - 오늘은 무엇을 할까. 정리하고 진행하자. 하루에 하나의 업무만 진행할 수는 없다. 내가 하고 싶은 것과 해야할 일을 분리하여 진행해볼 수 있도록 하자.
    - 오전은 오후보다 일반적으로 집중하기 어려운 상황이 많다고 느낀다. 그러므로 오전에는 하고 싶은 일을 하여 오전에 집중력을 잃지 않고, 보다 집중이 잘되는 시기인 오후에는 해아할 일을 하자. 16~17시까지는 해야할 일을 마무리하고 업무정리를 한 후에 하루동안 적은 것들에 대한 공부를 진행하자.
- 업무를 하며 느낀 점
    - 프로그래밍이란 행위 자체가 즐겁다.
    - 컴퓨터의 동작 원리를 이해해보는 것이 참 재밌다.
    - ChatGPT와 얘기를 통해 공부하고 모르는 것을 채워나가는 과정이 즐겁다.

### Eng

- abbreviate: 줄여 쓰다[축약하다]
- respective: 각자의, 각각의
- glossary: 용어 사전
- collision: 충돌, 부딪힘
- occurrence: 발생하는[존재하는/나타나는] 것, 발생, 존재, 나타남
- monotonic: 단조로운
- overall: 종합적인, 전체의
- thus far: 지금까지
- audit: 회계감사
- consecutive: 연이은
- within: 이내에
- unique: 유일무이한, 특별한, 고유한, 특유의
- voyage: 여행, 항해, 여행하다, 항해하다
- unable: 할 수 없는, 하지 못하는